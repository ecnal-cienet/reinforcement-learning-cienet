# Proximal Policy Optimization (PPO) å¯¦ä½œ - Pendulum-v1

## æ¦‚è¿°

é€™æ˜¯ä¸€å€‹ä½¿ç”¨ **Proximal Policy Optimization (PPO)** æ¼”ç®—æ³•ä¾†è§£æ±º OpenAI Gymnasium çš„ **Pendulum-v1** ç’°å¢ƒçš„å®Œæ•´å¯¦ä½œã€‚PPO æ˜¯ç¾ä»£å¼·åŒ–å­¸ç¿’çš„æ ¸å¿ƒæ¼”ç®—æ³•ï¼Œè¢«å»£æ³›æ‡‰ç”¨æ–¼æ©Ÿå™¨äººæ§åˆ¶ã€éŠæˆ² AIï¼Œä»¥åŠ**å¤§å‹èªè¨€æ¨¡å‹çš„å°é½Šè¨“ç·´ (RLHF)**ã€‚

**æ ¸å¿ƒçªç ´ï¼š** PPO çµåˆäº† **Actor-Critic æ¶æ§‹**ã€**é€£çºŒå‹•ä½œç©ºé–“è™•ç†**ã€ä»¥åŠ**ç©©å®šçš„ç­–ç•¥æ›´æ–°æ©Ÿåˆ¶**ï¼Œæ˜¯ç›®å‰å·¥æ¥­ç•Œæœ€å—æ­¡è¿çš„ RL æ¼”ç®—æ³•ä¹‹ä¸€ã€‚

**èˆ‡å‰å…©å€‹å°ˆæ¡ˆçš„é—œä¿‚ï¼š**
- `1_Q_Learning/`: å­¸ç¿’ Q å€¼ (åƒ¹å€¼ç‚ºåŸºç¤) - è¡¨æ ¼å‹
- `2_Cart_Pole_DQN/`: å­¸ç¿’ Q å€¼ (åƒ¹å€¼ç‚ºåŸºç¤) - æ·±åº¦å­¸ç¿’ + é›¢æ•£å‹•ä½œ
- `3_Pendulum/` **(æœ¬å°ˆæ¡ˆ)**: å­¸ç¿’ç­–ç•¥ (ç­–ç•¥ç‚ºåŸºç¤) - æ·±åº¦å­¸ç¿’ + **é€£çºŒå‹•ä½œ**

## ç’°å¢ƒèªªæ˜

### Pendulum-v1 (å€’ç«‹æ“º)

Pendulum æ˜¯ä¸€å€‹ç¶“å…¸çš„é€£çºŒæ§åˆ¶å•é¡Œï¼šä¸€å€‹æ“ºéŒ˜å¾éš¨æ©Ÿä½ç½®é–‹å§‹ï¼Œç›®æ¨™æ˜¯æ–½åŠ é©ç•¶çš„åŠ›çŸ© (Torque) ä¾†è®“æ“ºéŒ˜ä¿æŒåœ¨**æ­£ä¸Šæ–¹**çš„ç›´ç«‹ä½ç½®ã€‚

```
    â†‘ ç›®æ¨™ä½ç½®
    |
    |
    O â† æ¨ç´ (Pivot)
   /
  /  â† æ“ºéŒ˜ (Pendulum)
 â—

ç›®æ¨™ï¼šæ–½åŠ åŠ›çŸ©è®“æ“ºéŒ˜æ—‹è½‰åˆ°æ­£ä¸Šæ–¹ä¸¦ä¿æŒç©©å®š
```

### ç’°å¢ƒåƒæ•¸

- **ç‹€æ…‹ç©ºé–“ (State Space)**ï¼šé€£çºŒ 3 ç¶­å‘é‡
  - `cos(Î¸)`: æ“ºéŒ˜è§’åº¦çš„é¤˜å¼¦å€¼ (ç¯„åœ: -1 ~ 1)
  - `sin(Î¸)`: æ“ºéŒ˜è§’åº¦çš„æ­£å¼¦å€¼ (ç¯„åœ: -1 ~ 1)
  - `Î¸Ì‡`: æ“ºéŒ˜çš„è§’é€Ÿåº¦ (ç¯„åœ: -8 ~ 8 rad/s)

  > **ç‚ºä»€éº¼ä½¿ç”¨ cos/sin è€Œéè§’åº¦ï¼Ÿ** å› ç‚ºè§’åº¦æœ‰é€±æœŸæ€§ (0Â° = 360Â°)ï¼Œä½¿ç”¨ cos/sin å¯ä»¥è®“ç‹€æ…‹ç©ºé–“æ›´å¹³æ»‘ã€‚

- **å‹•ä½œç©ºé–“ (Action Space)**ï¼š**é€£çºŒ** 1 ç¶­å‘é‡
  - `torque`: æ–½åŠ çš„åŠ›çŸ© (ç¯„åœ: **-2 ~ 2**)
  - âš ï¸ **é—œéµå·®ç•°**ï¼šé€™æ˜¯**é€£çºŒå‹•ä½œ**ï¼Œä¸åƒ CartPole åªæœ‰ã€Œå·¦/å³ã€å…©å€‹é›¢æ•£é¸æ“‡

- **çå‹µå‡½æ•¸**ï¼š
  ```
  reward = -(Î¸Â² + 0.1 Ã— Î¸Ì‡Â² + 0.001 Ã— torqueÂ²)
  ```
  - æ‡²ç½°æ“ºéŒ˜åé›¢ç›´ç«‹ä½ç½® (Î¸Â²)
  - æ‡²ç½°éå¤§çš„è§’é€Ÿåº¦ (Î¸Ì‡Â²)
  - æ‡²ç½°éå¤§çš„åŠ›çŸ© (torqueÂ²ï¼Œé¼“å‹µç¯€èƒ½)
  - **ç¯„åœ**ï¼šç´„ -16.3 (æœ€å·®) ~ 0 (å®Œç¾)

- **çµ‚æ­¢æ¢ä»¶**ï¼š
  - æ²’æœ‰æå‰çµ‚æ­¢æ¢ä»¶
  - æ¯å€‹å›åˆå›ºå®š 200 æ­¥

- **æˆåŠŸæ¨™æº–**ï¼š
  - å¹³å‡çå‹µ > -200 è¡¨ç¤ºåŸºæœ¬æˆåŠŸ
  - å¹³å‡çå‹µ > -150 è¡¨ç¤ºè‰¯å¥½æ§åˆ¶

## åŸ·è¡Œæ–¹å¼

### å‰ç½®æ¢ä»¶

ç¢ºä¿å·²å•Ÿå‹•è™›æ“¬ç’°å¢ƒä¸¦å®‰è£ç›¸ä¾å¥—ä»¶ï¼š

```bash
source .venv/bin/activate
pip install -r requirements.txt
```

**ç‰¹åˆ¥æ³¨æ„ï¼š** æœ¬å°ˆæ¡ˆéœ€è¦ `tensorflow-probability` ä¾†è™•ç†é€£çºŒå‹•ä½œçš„æ©Ÿç‡åˆ†ä½ˆã€‚

### åŸ·è¡Œç¨‹å¼

```bash
python 3_Pendulum/pendulum.py
```

æˆ–è€…åœ¨ `3_Pendulum` ç›®éŒ„å…§åŸ·è¡Œï¼š

```bash
cd 3_Pendulum
python pendulum.py
```

## æ¼”ç®—æ³•æ ¸å¿ƒ

### å¾ DQN åˆ° PPO çš„æ¼”é€²

#### DQN çš„å…©å¤§å±€é™

1. **ç„¡æ³•è™•ç†é€£çºŒå‹•ä½œç©ºé–“**
   - DQN ä¾è³´ `argmax` æ“ä½œï¼š`action = argmax Q(s, a)`
   - åœ¨ Pendulum ä¸­ï¼Œå‹•ä½œæ˜¯ **-2.0 åˆ° 2.0 ä¹‹é–“çš„ä»»æ„å¯¦æ•¸**
   - ä½ ç„¡æ³•å°ã€Œç„¡é™å¤šå€‹ã€å‹•ä½œå– `max`

2. **åªèƒ½é–“æ¥å­¸ç¿’ç­–ç•¥**
   - DQN å­¸ç¿’çš„æ˜¯ã€ŒQ å€¼ã€ï¼Œç­–ç•¥ Ï€ æ˜¯å¾ Q å€¼ã€Œæ¨å°ã€å‡ºä¾†çš„
   - æˆ‘å€‘çœŸæ­£æƒ³è¦çš„æ˜¯ã€Œ**ç­–ç•¥ (Policy) æœ¬èº«**ã€

#### PPO çš„è§£æ±ºæ–¹æ¡ˆ

**æ ¸å¿ƒæ€æƒ³ï¼š** ç›´æ¥å­¸ç¿’ä¸€å€‹ã€Œ**ç­–ç•¥ç¶²è·¯ (Policy Network)**ã€ï¼Œç”¨ Ï€_Î¸(a|s) è¡¨ç¤ºã€‚

- **è¼¸å…¥**ï¼šç‹€æ…‹ s
- **è¼¸å‡º**ï¼šå‹•ä½œçš„**æ©Ÿç‡åˆ†ä½ˆ** (è€Œéå–®ä¸€å‹•ä½œ)
  - **é›¢æ•£å‹•ä½œ (CartPole)**ï¼š`[P(å·¦), P(å³)]` = `[0.3, 0.7]`
  - **é€£çºŒå‹•ä½œ (Pendulum)**ï¼šä¸€å€‹**å¸¸æ…‹åˆ†ä½ˆ** `N(Î¼, ÏƒÂ²)`
    - `Î¼` (å¹³å‡å€¼)ï¼šæœ€å¯èƒ½çš„å‹•ä½œ
    - `Ïƒ` (æ¨™æº–å·®)ï¼šæ¢ç´¢çš„ç¨‹åº¦

**ç¯„ä¾‹ï¼š**
```python
state = [cos(Î¸), sin(Î¸), Î¸Ì‡] = [0.8, 0.6, 1.2]
distribution = actor(state)  # â†’ N(Î¼=1.5, Ïƒ=0.3)
action = distribution.sample()  # å¾åˆ†ä½ˆä¸­æ¡æ¨£ â†’ å¯èƒ½å¾—åˆ° 1.7
```

### Actor-Critic æ¶æ§‹

PPO ä½¿ç”¨**å…©å€‹**ç¥ç¶“ç¶²è·¯ä¾†å”åŒå·¥ä½œï¼š

#### 1. æ¼”å“¡ (Actor) - ç­–ç•¥ç¶²è·¯ Ï€_Î¸

**å·¥ä½œï¼š** æ±ºç­–è€… (åšå‹•ä½œ)

**ç¶²è·¯çµæ§‹ï¼š** é›™é ­ MLP
```
Input (3)  â†’  FC(64) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ â”¬â†’ FC_mu(1)    â†’ tanh Ã— 2 â†’ Î¼
                                                 â””â†’ FC_sigma(1) â†’ softplus â†’ Ïƒ
```

**é—œéµè¨­è¨ˆï¼š**
1. **Î¼ é ­ (å¹³å‡å€¼)**ï¼š
   - ä½¿ç”¨ `tanh` å°‡è¼¸å‡ºå£“ç¸®åˆ° [-1, 1]
   - å†ä¹˜ä»¥ 2 â†’ ç¯„åœè®Šç‚º [-2, 2] (ç¬¦åˆç’°å¢ƒè¦æ±‚)

2. **Ïƒ é ­ (æ¨™æº–å·®)**ï¼š
   - ä½¿ç”¨ `softplus` ç¢ºä¿ Ïƒ > 0 (æ¨™æº–å·®å¿…é ˆæ˜¯æ­£æ•¸)
   - åŠ ä¸Š 1e-5 é¿å…æ•¸å€¼ä¸ç©©å®š

**è¼¸å‡ºï¼š** `tfp.distributions.Normal(loc=Î¼, scale=Ïƒ)`

**ç¨‹å¼ç¢¼ï¼š**
```python
class Actor(nnx.Module):
    def __call__(self, x: jax.Array) -> tfd.Normal:
        x = nnx.relu(self.fc1(x))
        x = nnx.relu(self.fc2(x))

        mu = jnp.tanh(self.fc_mu(x)) * 2.0      # å¹³å‡å€¼ [-2, 2]
        sigma = nnx.softplus(self.fc_sigma(x)) + 1e-5  # æ¨™æº–å·® > 0

        return tfd.Normal(loc=mu, scale=sigma)   # å›å‚³æ©Ÿç‡åˆ†ä½ˆ
```

#### 2. è©•è«–å®¶ (Critic) - åƒ¹å€¼ç¶²è·¯ V_Ï†

**å·¥ä½œï¼š** è©•åˆ†è€… (æä¾›åŸºç·š)

**ç¶²è·¯çµæ§‹ï¼š** æ¨™æº– MLP
```
Input (3)  â†’  FC(64) â†’ ReLU â†’ FC(64) â†’ ReLU â†’ FC_out(1) â†’ V(s)
```

**è¼¸å‡ºï¼š** ä¸€å€‹æ•¸å­—ï¼Œä»£è¡¨ã€Œåœ¨ç‹€æ…‹ sï¼Œæˆ‘é æœŸèƒ½ç²å¾—çš„ç¸½çå‹µã€

**ç¨‹å¼ç¢¼ï¼š**
```python
class Critic(nnx.Module):
    def __call__(self, x: jax.Array) -> jax.Array:
        x = nnx.relu(self.fc1(x))
        x = nnx.relu(self.fc2(x))
        return self.fc_out(x)  # è¼¸å‡º V(s)
```

### PPO çš„ä¸‰å¤§æ ¸å¿ƒæŠ€è¡“

#### æŠ€è¡“ 1ï¼šAdvantage (å„ªå‹¢å‡½æ•¸)

**å•é¡Œï¼š** REINFORCE ä½¿ç”¨ã€Œçµ•å°ç¸½åˆ†ã€ä½œç‚ºå­¸ç¿’è¨Šè™Ÿ â†’ é›œè¨Šå¤ªé«˜

**è§£æ±ºæ–¹æ¡ˆï¼š** ä½¿ç”¨ã€Œç›¸å°åˆ†æ•¸ã€

```
Advantage(s, a) = å¯¦éš›æ‹¿åˆ°çš„åˆ†æ•¸ - Critic é æœŸçš„åˆ†æ•¸
A(s, a) = Q(s, a) - V(s)
```

**è¨Šè™Ÿè§£è®€ï¼š**
- `A > 0`ï¼šè¡¨ç¾**æ¯”é æœŸå¥½** â†’ å¢åŠ é€™å€‹å‹•ä½œçš„æ©Ÿç‡ âœ…
- `A < 0`ï¼šè¡¨ç¾**æ¯”é æœŸå·®** â†’ é™ä½é€™å€‹å‹•ä½œçš„æ©Ÿç‡ âŒ
- `A â‰ˆ 0`ï¼šè¡¨ç¾**ç¬¦åˆé æœŸ** â†’ ä¸æ”¹è®Š

#### æŠ€è¡“ 2ï¼šGAE (Generalized Advantage Estimation)

**å•é¡Œï¼š** å¦‚ä½•æº–ç¢ºè¨ˆç®— Advantageï¼Ÿ

**æ–¹æ¡ˆï¼š** ä½¿ç”¨ GAEï¼Œé€™æ˜¯ä¸€ç¨®ã€Œå¹³æ»‘ã€çš„ Advantage è¨ˆç®—æ–¹æ³•

**GAE å…¬å¼ (å¾å¾Œå¾€å‰éè¿´)ï¼š**
```python
for t in reversed(range(N)):
    # 1. è¨ˆç®— TD èª¤å·®
    delta_t = reward_t + Î³ Ã— V(s_{t+1}) - V(s_t)

    # 2. è¨ˆç®— GAE (éè¿´)
    A_t = delta_t + Î³ Ã— Î» Ã— A_{t+1}

    # 3. è¨ˆç®— Return (Critic çš„å­¸ç¿’ç›®æ¨™)
    Return_t = A_t + V(s_t)
```

**è¶…åƒæ•¸ï¼š**
- `Î³` (GAMMA = 0.99)ï¼šæŠ˜æ‰£å› å­ (å°æœªä¾†çå‹µçš„é‡è¦–ç¨‹åº¦)
- `Î»` (GAE_LAMBDA = 0.95)ï¼šGAE çš„å¹³æ»‘åƒæ•¸
  - `Î» = 0`ï¼šåªçœ‹ä¸€æ­¥ (ä½è®Šç•°æ•¸ï¼Œé«˜åå·®)
  - `Î» = 1`ï¼šçœ‹åˆ°åº• (é«˜è®Šç•°æ•¸ï¼Œä½åå·®)
  - `Î» = 0.95`ï¼šæŠ˜è¡·æ–¹æ¡ˆ â­

**æœ€çµ‚å„ªåŒ–ï¼šAdvantage æ¨™æº–åŒ–**
```python
advantages = (advantages - mean) / (std + 1e-8)
```
è®“ Advantage çš„å¹³å‡å€¼ç‚º 0ï¼Œæ¨™æº–å·®ç‚º 1 â†’ è¨“ç·´æ›´ç©©å®š

#### æŠ€è¡“ 3ï¼šPPO-Clip (é™åˆ¶æ›´æ–°æ­¥ä¼)

**å•é¡Œï¼š** Actor-Critic è¨“ç·´ä¸ç©©å®šï¼Œå¯èƒ½ã€Œä¸€æ­¥èµ°å¤ªå¤§ã€å°è‡´ç­–ç•¥å´©æ½°

**è§£æ±ºæ–¹æ¡ˆï¼š** PPO-Clip å¢åŠ ã€Œå®‰å…¨é–ã€

**æ ¸å¿ƒæ¦‚å¿µï¼šç­–ç•¥æ¯”ä¾‹ (Policy Ratio)**
```
Ratio = Ï€_new(a|s) / Ï€_old(a|s)
```
- `Ratio â‰ˆ 1`ï¼šæ–°èˆŠç­–ç•¥ç›¸ä¼¼ (å®‰å…¨)
- `Ratio >> 1` æˆ– `Ratio << 1`ï¼šæ–°èˆŠç­–ç•¥å·®ç•°å¤ªå¤§ (å±éšª)

**PPO-Clip Loss å‡½æ•¸ï¼š**
```python
# è¨ˆç®—å…©ç¨® Loss
loss_unclipped = Advantage Ã— Ratio
loss_clipped = Advantage Ã— clip(Ratio, 1-Îµ, 1+Îµ)

# å–è¼ƒå°å€¼ (æ‚²è§€åŸå‰‡)
loss = -mean(minimum(loss_unclipped, loss_clipped))
```

**CLIP_EPSILON = 0.2 çš„å«ç¾©ï¼š**
- Ratio è¢«é™åˆ¶åœ¨ [0.8, 1.2] ç¯„åœå…§
- å³ä½¿ Advantage å¾ˆå¤§ï¼Œç­–ç•¥ä¹Ÿ**ä¸èƒ½**ä¸€æ¬¡æ›´æ–°è¶…é 20%
- ç¢ºä¿è¨“ç·´ç©©å®š

**è¦–è¦ºåŒ–ï¼š**
```
Advantage > 0 (å¥½å‹•ä½œ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å…è¨±å¢åŠ æ©Ÿç‡ï¼Œä½†ä¸è¶…é 20%  â”‚  â† Clip ä¸Šé™ (1.2)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ­£å¸¸æ›´æ–°å€é–“ [0.8, 1.2]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å…è¨±æ¸›å°‘æ©Ÿç‡ï¼Œä½†ä¸è¶…é 20%  â”‚  â† Clip ä¸‹é™ (0.8)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Advantage < 0 (å£å‹•ä½œ) - åéä¾†
```

## PPO è¨“ç·´æµç¨‹

### On-Policy vs Off-Policy

| ç‰¹æ€§ | Off-Policy (DQN) | On-Policy (PPO) |
|------|-----------------|----------------|
| **è³‡æ–™ä¾†æº** | ä»»ä½•èˆŠç­–ç•¥ | å¿…é ˆæ˜¯**ç•¶å‰**ç­–ç•¥ |
| **ç¶“é©—å›æ”¾** | âœ… Replay Buffer (å¯é‡è¤‡ä½¿ç”¨) | âŒ Rollout Buffer (ç”¨å®Œå³ä¸Ÿ) |
| **è¨“ç·´ç©©å®šæ€§** | è¼ƒé›£ (éœ€è¦ Target Network) | è¼ƒæ˜“ (ç­–ç•¥æ›´æ–°æ›´å¹³æ»‘) |
| **æ¨£æœ¬æ•ˆç‡** | é«˜ (ä¸€ç­†è³‡æ–™ç”¨å¤šæ¬¡) | ä½ (ä¸€ç­†è³‡æ–™åªç”¨ä¸€æ¬¡) |

**ç‚ºä»€éº¼ PPO æ˜¯ On-Policyï¼Ÿ**
- PPO çš„ Loss è¨ˆç®—éœ€è¦ã€ŒèˆŠç­–ç•¥çš„ log æ©Ÿç‡ã€
- å¦‚æœè³‡æ–™ä¾†è‡ªã€Œå¤ªèˆŠã€çš„ç­–ç•¥ï¼ŒRatio æœƒå¤±çœŸ
- å› æ­¤ï¼ŒPPO å¿…é ˆåœ¨**æ”¶é›†å®Œè³‡æ–™å¾Œç«‹åˆ»å­¸ç¿’ï¼Œç„¶å¾Œä¸Ÿæ£„**

### RolloutBuffer (On-Policy å„²å­˜å€)

```python
class RolloutBuffer:
    def add(self, state, action, reward, log_prob, value, done):
        # å„²å­˜ä¸€æ­¥çš„ç¶“é©—

    def calculate_advantages_and_returns(self, last_value, gamma, gae_lambda):
        # è¨ˆç®— GAE å’Œ Returns (å¾å¾Œå¾€å‰)

    def get_data_for_learning(self):
        # è½‰æ›ç‚º JAX é™£åˆ—ä¾›è¨“ç·´ä½¿ç”¨

    def clear(self):
        # å­¸ç¿’å®Œç•¢å¾Œï¼Œæ¸…ç©ºæ‰€æœ‰è³‡æ–™
```

### PPO å››éšæ®µç”Ÿå‘½é€±æœŸ

```python
while total_steps < MAX_STEPS:
    # ========== éšæ®µ 1: æ”¶é›† (Rollout) ==========
    for _ in range(ROLLOUT_STEPS):  # ä¾‹å¦‚ 2048 æ­¥
        # 1. é¸æ“‡å‹•ä½œ
        action, value, log_prob = agent.select_action(state)

        # 2. èˆ‡ç’°å¢ƒäº’å‹•
        next_state, reward, done, _, _ = env.step(action)

        # 3. å„²å­˜åˆ° Buffer
        buffer.add(state, action, reward, log_prob, value, done)

        state = next_state

    # ========== éšæ®µ 2: è¨ˆç®—å­¸ç¿’ç›®æ¨™ (GAE) ==========
    # å–å¾—ã€Œæœ€å¾Œä¸€æ­¥ã€çš„ V å€¼
    last_value = critic(state)

    # è¨ˆç®—æ‰€æœ‰æ­¥é©Ÿçš„ Advantages å’Œ Returns
    buffer.calculate_advantages_and_returns(last_value, GAMMA, GAE_LAMBDA)

    # ========== éšæ®µ 3: å­¸ç¿’ (Learn) ==========
    # å–å¾—æ‰€æœ‰è³‡æ–™
    states, actions, log_probs_old, advantages, returns = buffer.get_data_for_learning()

    # åè¦†è¨“ç·´ K æ¬¡ (TRAIN_EPOCHS = 10)
    for epoch in range(TRAIN_EPOCHS):
        # æ‰“äº‚è³‡æ–™
        indices = random.permutation(ROLLOUT_STEPS)

        # åˆ†æ‰¹è¨“ç·´ (BATCH_SIZE = 64)
        for batch_indices in batches(indices, BATCH_SIZE):
            # è¨“ç·´ Critic (æœ€å°åŒ– MSE)
            train_critic(batch_states, batch_returns)

            # è¨“ç·´ Actor (PPO-Clip Loss)
            train_actor(batch_states, batch_actions,
                       batch_log_probs_old, batch_advantages)

    # ========== éšæ®µ 4: ä¸Ÿæ£„ (Discard) ==========
    buffer.clear()  # æ¸…ç©ºæ‰€æœ‰ã€ŒèˆŠç­–ç•¥ã€çš„è³‡æ–™
```

## è¶…åƒæ•¸è¨­å®š

| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `STATE_DIM` | 3 | ç‹€æ…‹ç©ºé–“ç¶­åº¦ (cos Î¸, sin Î¸, Î¸Ì‡) |
| `ACTION_DIM` | 1 | å‹•ä½œç©ºé–“ç¶­åº¦ (torque) |
| `NUM_TOTAL_TIMESTEPS` | 100,000 | ç¸½è¨“ç·´æ­¥æ•¸ |
| `ROLLOUT_STEPS` | 2,048 | æ¯æ¬¡æ”¶é›†çš„æ­¥æ•¸ (N) |
| `TRAIN_EPOCHS` | 10 | æ¯æ‰¹è³‡æ–™è¨“ç·´çš„è¼ªæ•¸ (K) |
| `BATCH_SIZE` | 64 | Mini-batch å¤§å° |
| `GAMMA` | 0.99 | æŠ˜æ‰£å› å­ (Î³) |
| `GAE_LAMBDA` | 0.95 | GAE å¹³æ»‘åƒæ•¸ (Î») |
| `CLIP_EPSILON` | 0.2 | PPO è£å‰ªåƒæ•¸ (Îµ) |
| `ACTOR_LR` | 3e-4 | Actor å­¸ç¿’ç‡ |
| `CRITIC_LR` | 1e-3 | Critic å­¸ç¿’ç‡ |

**å­¸ç¿’ç‡é¸æ“‡ï¼š**
- Critic çš„å­¸ç¿’ç‡ (1e-3) æ¯” Actor (3e-4) é«˜
- åŸå› ï¼šCritic éœ€è¦å¿«é€Ÿå­¸æœƒè©•ä¼°ç‹€æ…‹ï¼Œç‚º Actor æä¾›æº–ç¢ºçš„åŸºç·š

## é æœŸè¼¸å‡º

### è¨“ç·´éç¨‹

```
é–‹å§‹ PPO è¨“ç·´...

--- æ­£åœ¨æ”¶é›† 2048 æ­¥çš„è³‡æ–™ ---
...æ­£åœ¨è¨ˆç®— GAE (Advantages) å’Œ Returns...
...é–‹å§‹ 10 å€‹ Epochs çš„å­¸ç¿’...
ç›®å‰ç¸½æ­¥æ•¸: 2048/100000

--- æ­£åœ¨æ”¶é›† 2048 æ­¥çš„è³‡æ–™ ---
...æ­£åœ¨è¨ˆç®— GAE (Advantages) å’Œ Returns...
...é–‹å§‹ 10 å€‹ Epochs çš„å­¸ç¿’...
ç›®å‰ç¸½æ­¥æ•¸: 4096/100000

--- æ­£åœ¨æ”¶é›† 2048 æ­¥çš„è³‡æ–™ ---
...æ­£åœ¨è¨ˆç®— GAE (Advantages) å’Œ Returns...
...é–‹å§‹ 10 å€‹ Epochs çš„å­¸ç¿’...
ç›®å‰ç¸½æ­¥æ•¸: 6144/100000

...

ç›®å‰ç¸½æ­¥æ•¸: 100000/100000
--- è¨“ç·´å®Œæˆï¼ ---
```

**è§£è®€ï¼š**
- æ¯æ¬¡æ”¶é›† 2048 æ­¥
- è¨ˆç®— GAE
- è¨“ç·´ 10 å€‹ Epochs (æ¯å€‹ Epoch ä½¿ç”¨æ‰€æœ‰ 2048 ç­†è³‡æ–™ï¼Œåˆ†æˆå¤šå€‹ batch)
- ä¸Ÿæ£„è³‡æ–™ä¸¦é–‹å§‹ä¸‹ä¸€è¼ª

### è©•ä¼°è¨“ç·´æ•ˆæœ

è¨“ç·´å®Œæˆå¾Œï¼Œå¯ä»¥æ‰‹å‹•è©•ä¼° Agent çš„è¡¨ç¾ï¼š

```python
# åœ¨ main() å‡½æ•¸æœ€å¾Œæ·»åŠ 
env = gym.make("Pendulum-v1", render_mode="human")
state, _ = env.reset()

for _ in range(1000):
    action, _, _ = agent.select_action(state)
    state, reward, done, _, _ = env.step(action)
    if done:
        state, _ = env.reset()
```

**æˆåŠŸçš„æ¨™èªŒï¼š**
- æ“ºéŒ˜èƒ½å¿«é€Ÿæ—‹è½‰åˆ°æ­£ä¸Šæ–¹
- åœ¨æ­£ä¸Šæ–¹ä¿æŒç©©å®š (å°å¹…éœ‡ç›ª)
- å¹³å‡çå‹µ > -200

## æ ¸å¿ƒç¨‹å¼ç¢¼è§£æ

### 1. Actor çš„ select_action (JAX â†” NumPy æ©‹æ¨‘)

```python
def select_action(self, state: np.ndarray):
    # NumPy â†’ JAX (å¢åŠ  batch ç¶­åº¦)
    state_jnp = jnp.asarray(state[np.newaxis, :], dtype=jnp.float32)

    # å‘¼å« Actor â†’ å–å¾—æ©Ÿç‡åˆ†ä½ˆ
    action_dist = self.actor(state_jnp)  # N(Î¼, Ïƒ)

    # å‘¼å« Critic â†’ å–å¾—åŸºç·š
    value = self.critic(state_jnp)  # V(s)
    value = jax.lax.stop_gradient(value)  # é˜»æ­¢æ¢¯åº¦å›å‚³

    # å¾åˆ†ä½ˆä¸­æ¡æ¨£å‹•ä½œ
    action = action_dist.sample(seed=rng_key)

    # è¨ˆç®— log æ©Ÿç‡ (PPO å¿…é ˆ)
    log_prob = action_dist.log_prob(action)

    # JAX â†’ NumPy (ç§»é™¤ batch ç¶­åº¦)
    return action.flatten(), value.flatten(), log_prob.flatten()
```

**ç‚ºä»€éº¼éœ€è¦ log_probï¼Ÿ**
- PPO éœ€è¦è¨ˆç®— `Ratio = exp(log_prob_new - log_prob_old)`
- å¿…é ˆåœ¨ã€Œæ¡æ¨£ç•¶ä¸‹ã€è¨˜éŒ„ log_prob_old

### 2. Critic è¨“ç·´ (MSE Loss)

```python
def critic_loss_fn(critic_model: Critic):
    values_pred = critic_model(batch_states)  # é æ¸¬çš„ V(s)
    loss = jnp.mean((batch_returns - values_pred.flatten()) ** 2)
    return loss

# è¨ˆç®—æ¢¯åº¦ä¸¦æ›´æ–°
_, critic_grads = nnx.value_and_grad(critic_loss_fn)(self.critic)
self.critic_optimizer.update(critic_grads)
```

**ç›®æ¨™ï¼š** è®“ V(s) ç›¡å¯èƒ½æ¥è¿‘ã€Œå¯¦éš›ç¸½åˆ†ã€(Returns)

### 3. Actor è¨“ç·´ (PPO-Clip Loss)

```python
def actor_loss_fn(actor_model: Actor):
    # 1. å–å¾—æ–°çš„ log æ©Ÿç‡
    action_dist_new = actor_model(batch_states)
    log_probs_new = action_dist_new.log_prob(batch_actions)

    # 2. è¨ˆç®— Ratio
    ratio = jnp.exp(log_probs_new - batch_log_probs_old)

    # 3. è¨ˆç®—æœªè£å‰ªçš„ Loss
    loss_unclipped = batch_advantages * ratio

    # 4. è¨ˆç®—è£å‰ªçš„ Loss
    ratio_clipped = jnp.clip(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON)
    loss_clipped = batch_advantages * ratio_clipped

    # 5. å–æœ€å°å€¼ (æ‚²è§€åŸå‰‡)
    loss = -jnp.mean(jnp.minimum(loss_unclipped, loss_clipped))
    return loss

# è¨ˆç®—æ¢¯åº¦ä¸¦æ›´æ–°
_, actor_grads = nnx.value_and_grad(actor_loss_fn)(self.actor)
self.actor_optimizer.update(actor_grads)
```

**é—œéµï¼š** åŠ è² è™Ÿ `-` æ˜¯å› ç‚º Adam åªèƒ½ã€Œæœ€å°åŒ–ã€ï¼Œè€Œæˆ‘å€‘è¦ã€Œæœ€å¤§åŒ–ã€Advantage

## Q-Learning â†’ DQN â†’ PPO æ¼”é€²ç¸½çµ

| ç‰¹æ€§ | Q-Learning | DQN | PPO |
|------|-----------|-----|-----|
| **å­¸ç¿’å°è±¡** | Q å€¼ (åƒ¹å€¼) | Q å€¼ (åƒ¹å€¼) | ç­–ç•¥ (Policy) |
| **å‡½æ•¸è¿‘ä¼¼** | âŒ Q-Table | âœ… ç¥ç¶“ç¶²è·¯ | âœ… ç¥ç¶“ç¶²è·¯ (Actor + Critic) |
| **å‹•ä½œç©ºé–“** | é›¢æ•£ | é›¢æ•£ | **é€£çºŒ + é›¢æ•£** |
| **ç­–ç•¥é¡å‹** | Off-Policy | Off-Policy | **On-Policy** |
| **ç¶“é©—å›æ”¾** | âŒ | âœ… Replay Buffer | âŒ (Rollout Buffer) |
| **ç©©å®šæŠ€è¡“** | âŒ | Target Network | **PPO-Clip + GAE** |
| **å„ªå‹¢å‡½æ•¸** | âŒ | âŒ | âœ… |
| **é©ç”¨å ´æ™¯** | å°ç‹€æ…‹ç©ºé–“ | å¤§ç‹€æ…‹ç©ºé–“ + é›¢æ•£å‹•ä½œ | **ä»»ä½•å ´æ™¯** (æœ€é€šç”¨) |

## PPO çš„å¯¦éš›æ‡‰ç”¨

PPO æ˜¯ç›®å‰å·¥æ¥­ç•Œæœ€å—æ­¡è¿çš„ RL æ¼”ç®—æ³•ï¼Œæ‡‰ç”¨åŒ…æ‹¬ï¼š

1. **æ©Ÿå™¨äººæ§åˆ¶**
   - æ©Ÿæ¢°è‡‚æŠ“å–
   - å››è¶³æ©Ÿå™¨äººè¡Œèµ°
   - ç„¡äººæ©Ÿé£›è¡Œ

2. **éŠæˆ² AI**
   - OpenAI Five (Dota 2)
   - AlphaStar (StarCraft II)
   - å„ç¨®é€£çºŒæ§åˆ¶éŠæˆ²

3. **å¤§å‹èªè¨€æ¨¡å‹å°é½Š (RLHF)**
   - ChatGPT çš„è¨“ç·´
   - Claude çš„è¨“ç·´
   - **GRPO** (Group Relative Policy Optimization) æ˜¯ PPO çš„è®Šé«”

4. **è‡ªå‹•é§•é§›**
   - è·¯å¾‘è¦åŠƒ
   - é€Ÿåº¦æ§åˆ¶

## é€²éšä¸»é¡Œ

### PPO çš„è®Šé«”

1. **PPO-Penalty**
   - ä½¿ç”¨ KL æ•£åº¦æ‡²ç½°ä»£æ›¿ Clip
   - `Loss = Advantage - Î² Ã— KL(Ï€_new || Ï€_old)`

2. **GRPO** (ç”¨æ–¼ LLM)
   - Group Relative Policy Optimization
   - å°ˆç‚ºå¤§å‹èªè¨€æ¨¡å‹è¨­è¨ˆçš„ PPO è®Šé«”

### é€²ä¸€æ­¥å„ªåŒ–

1. **Vectorized Environments**
   - åŒæ™‚é‹è¡Œå¤šå€‹ç’°å¢ƒå‰¯æœ¬
   - åŠ é€Ÿè³‡æ–™æ”¶é›†

2. **Normalization**
   - ç‹€æ…‹æ¨™æº–åŒ–
   - çå‹µæ¨™æº–åŒ–

3. **Learning Rate Scheduling**
   - å­¸ç¿’ç‡éæ¸›
   - æé«˜è¨“ç·´å¾ŒæœŸçš„ç©©å®šæ€§

## Flax NNX é—œéµ API ç¸½çµ

### 1. å¤šç¶²è·¯ç®¡ç†

```python
# å»ºç«‹å…©å€‹ç¨ç«‹çš„ç¶²è·¯
actor_key, critic_key = jax.random.split(rng_key)
self.actor = Actor(..., rngs=nnx.Rngs(actor_key))
self.critic = Critic(..., rngs=nnx.Rngs(critic_key))
```

### 2. å¤šå„ªåŒ–å™¨ç®¡ç†

```python
# æ¯å€‹ç¶²è·¯æœ‰è‡ªå·±çš„å„ªåŒ–å™¨
self.actor_optimizer = nnx.Optimizer(self.actor, optax.adam(3e-4))
self.critic_optimizer = nnx.Optimizer(self.critic, optax.adam(1e-3))
```

### 3. æ¢¯åº¦é˜»æ–·

```python
# åœ¨ select_action æ™‚é˜»æ­¢ Critic çš„æ¢¯åº¦
value = self.critic(state_jnp)
value = jax.lax.stop_gradient(value)  # ä¸è¨“ç·´ Critic
```

### 4. RNG æµç®¡ç†

```python
# å»ºç«‹ RNG æµ
self.rng_stream = nnx.Rngs(jax.random.PRNGKey(42))

# åœ¨éœ€è¦éš¨æ©Ÿæ€§æ™‚å–å¾—æ–°å¯†é‘°
rng_key = self.rng_stream.sampler()
action = action_dist.sample(seed=rng_key)
```

## åƒè€ƒè³‡æ–™

- Schulman et al. (2017). "Proximal Policy Optimization Algorithms" ([arXiv:1707.06347](https://arxiv.org/abs/1707.06347))
- Schulman et al. (2015). "High-Dimensional Continuous Control Using Generalized Advantage Estimation" ([arXiv:1506.02438](https://arxiv.org/abs/1506.02438))
- Sutton & Barto, "Reinforcement Learning: An Introduction" (Chapter 13: Policy Gradient Methods)
- [Gymnasium Pendulum-v1 Documentation](https://gymnasium.farama.org/environments/classic_control/pendulum/)
- [OpenAI Spinning Up - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

## æ­å–œå®Œæˆ RL å­¸ç¿’ä¹‹æ—…ï¼ ğŸ‰

ä½ å·²ç¶“å®Œæˆäº†å¾è¡¨æ ¼å‹æ–¹æ³• (Q-Learning) â†’ æ·±åº¦åƒ¹å€¼å­¸ç¿’ (DQN) â†’ æ·±åº¦ç­–ç•¥å­¸ç¿’ (PPO) çš„å®Œæ•´æ—…ç¨‹ã€‚é€™ä¸‰å€‹å°ˆæ¡ˆæ¶µè“‹äº†ç¾ä»£å¼·åŒ–å­¸ç¿’çš„æ ¸å¿ƒæ¦‚å¿µå’ŒæŠ€è¡“ã€‚

**ä¸‹ä¸€æ­¥å»ºè­°ï¼š**
1. å˜—è©¦å°‡ PPO æ‡‰ç”¨åˆ°å…¶ä»– Gymnasium ç’°å¢ƒ
2. æ¢ç´¢ Multi-Agent RL (å¤šæ™ºèƒ½é«”å¼·åŒ–å­¸ç¿’)
3. ç ”ç©¶ Offline RL (é›¢ç·šå¼·åŒ–å­¸ç¿’)
4. æ·±å…¥äº†è§£ RLHF å’Œ LLM å°é½ŠæŠ€è¡“
