## <a name="module-3-deep-q-networks-dqn"></a>æ¨¡çµ„ä¸‰ï¼šæ·±åº¦å¼·åŒ–å­¸ç¿’ (DQN)

[â¬…ï¸ ä¸Šä¸€ç« ï¼šæ¨¡çµ„äºŒ - Q-Learning](../2.Q_Learning/README.md) | [è¿”å›ç›®éŒ„](../README.md) | [ä¸‹ä¸€ç« ï¼šæ¨¡çµ„å›› - Policy Gradients â¡ï¸](../4.Policy_Gradients/README.md)

---

æœ¬æ¨¡çµ„æ˜¯å¾ã€Œè¡¨æ ¼å‹ã€æ–¹æ³•åˆ°ã€Œæ·±åº¦å­¸ç¿’ã€æ–¹æ³•çš„é—œéµé£›èºã€‚

### 3.1 å‡½æ•¸è¿‘ä¼¼ (Function Approximation)

**1. ã€Œè¡¨æ ¼ã€çš„è©›å’’ (Curse of Dimensionality)**
* **å•é¡Œ**ï¼šQ-Table (æ¨¡çµ„äºŒ) åªèƒ½ç”¨åœ¨ç‹€æ…‹ç©ºé–“**æ¥µå°** (ä¾‹å¦‚ 16 å€‹) çš„å•é¡Œä¸Šã€‚
* **ç¯„ä¾‹**ï¼šå¦‚æœç© Atari éŠæˆ²ï¼Œç‹€æ…‹æ˜¯ $84 \times 84$ åƒç´ çš„ç•«é¢ã€‚ç‹€æ…‹çš„ç¸½æ•¸ (ä¾‹å¦‚ $4^{84 \times 84}$) é è¶…å®‡å®™ä¸­çš„åŸå­æ•¸ã€‚
* **çµè«–**ï¼šæˆ‘å€‘**ä¸å¯èƒ½**å»ºç«‹ä¸€å€‹ Q-Table ä¾†ã€Œå„²å­˜ã€æ‰€æœ‰ Q å€¼ã€‚

**2. è§£æ±ºæ–¹æ¡ˆï¼šå‡½æ•¸è¿‘ä¼¼ (Function Approximation)**
* **æ ¸å¿ƒæ€æƒ³**ï¼šæˆ‘å€‘ä¸è¦ã€Œ**å„²å­˜ (store)**ã€æ‰€æœ‰çš„ Q å€¼ï¼Œè€Œæ˜¯è¨“ç·´ä¸€å€‹ã€Œ**ä¼°ç®—å™¨ (estimator)**ã€ (ä¸€å€‹å‡½æ•¸)ï¼Œè®“å®ƒä¾†ã€Œ**ä¼°ç®— (estimate)**ã€Q å€¼ã€‚
* **æˆ‘å€‘çš„ä¼°ç®—å™¨**ï¼š**æ·±åº¦ç¥ç¶“ç¶²è·¯ (Deep Neural Network)**ã€‚

**3. Deep Q-Network (DQN)**
* DQN å°±æ˜¯ä¸€å€‹ç¥ç¶“ç¶²è·¯ï¼Œå®ƒè¢«è¨“ç·´ä¾†**æ‰®æ¼”ã€ŒQ-Tableã€çš„è§’è‰²**ã€‚
* **è¼¸å…¥ (Input)**ï¼šç‹€æ…‹ $S$ (ä¾‹å¦‚ï¼šéŠæˆ²ç•«é¢æˆ– CartPole çš„ 4 å€‹æ•¸å­—)ã€‚
* **è¼¸å‡º (Output)**ï¼šä¸€å€‹å‘é‡ï¼Œä»£è¡¨**æ‰€æœ‰å¯èƒ½å‹•ä½œ**çš„ Q å€¼ã€‚
    * `Q_Network(S)` $\rightarrow$ `[Q(S, a_1), Q(S, a_2), ...]`

**4. å‡½æ•¸è¿‘ä¼¼çš„å¥½è™•**
1.  **è¨˜æ†¶é«”æ•ˆç‡**ï¼šä¸€å€‹å¹¾ç™¾è¬åƒæ•¸çš„ç¶²è·¯ï¼Œå¯ä»¥ç‚ºã€Œç„¡é™ã€çš„ç‹€æ…‹ç©ºé–“ä¼°ç®— Q å€¼ã€‚
2.  **æ³›åŒ–èƒ½åŠ› (Generalization)**ï¼š
    * åœ¨ Q-Table ä¸­ï¼Œç‹€æ…‹ `(1,1)` å’Œ `(1,2)` æ˜¯å…©å€‹**ç¨ç«‹**çš„æ¢ç›®ã€‚
    * åœ¨ç¥ç¶“ç¶²è·¯ä¸­ï¼Œ`state (1,1)` å’Œ `state (1,2)` æ˜¯**éå¸¸ç›¸ä¼¼ (similar)** çš„è¼¸å…¥ã€‚
    * ç¶²è·¯åœ¨ `(1,1)` å­¸åˆ°çš„ç¶“é©— (ä¾‹å¦‚ã€Œå¾€å³èµ°å¾ˆå¥½ã€)ï¼Œæœƒ**è‡ªå‹•ã€Œæ³›åŒ–ã€**åˆ° `(1,2)`ï¼Œè®“å®ƒçŒœåˆ°ã€Œåœ¨ (1,2) å¾€å³èµ°å¯èƒ½ä¹Ÿä¸éŒ¯ã€ã€‚

### 3.2 ç‚ºä»€éº¼ DQN ç„¡æ³•è™•ç†é€£çºŒå‹•ä½œï¼Ÿ

é€™æ˜¯ DQN çš„è‡´å‘½é™åˆ¶ï¼Œç›´æ¥å°è‡´äº†ç­–ç•¥æ¢¯åº¦æ–¹æ³• (æ¨¡çµ„å››) çš„éœ€æ±‚ã€‚

**æ•¸å­¸åŸå› **ï¼š
$$
a^* = \arg\max_{a} Q(s, a) \quad \leftarrow \text{DQN çš„å‹•ä½œé¸æ“‡}
$$

**å•é¡Œåˆ†æ**ï¼š
* **é›¢æ•£å‹•ä½œç©ºé–“ (ä¾‹å¦‚ CartPole)**ï¼šå‹•ä½œ = {å·¦, å³}
  * ç¶²è·¯è¼¸å‡ºï¼š`[Q(s, å·¦), Q(s, å³)]` = `[2.3, 5.1]`
  * é¸æ“‡ï¼š`argmax` â†’ å³ (ç´¢å¼• 1)
  * âœ… å¯è¡Œï¼

* **é€£çºŒå‹•ä½œç©ºé–“ (ä¾‹å¦‚æ©Ÿå™¨äººé—œç¯€è§’åº¦)**ï¼šå‹•ä½œ = [-180Â°, +180Â°]
  * æœ‰**ç„¡é™å¤šå€‹**å¯èƒ½çš„å‹•ä½œå€¼
  * å¦‚ä½•è¨ˆç®— $\max_{a \in [-180, 180]} Q(s, a)$ï¼Ÿ
  * âŒ ç„¡æ³•æšèˆ‰æ‰€æœ‰å¯èƒ½ï¼

**å¸¸è¦‹çš„èª¤è§£èˆ‡éŒ¯èª¤å˜—è©¦**ï¼š
1. **ã€Œé›¢æ•£åŒ–ã€é€£çºŒå‹•ä½œ**ï¼šå°‡ [-180Â°, 180Â°] åˆ‡æˆ 360 ä»½
   * å•é¡Œï¼šå‹•ä½œç©ºé–“è®Šæˆ 360 ç¶­ï¼Œç¶²è·¯éœ€è¦è¼¸å‡º 360 å€‹ Q å€¼
   * å¦‚æœæ˜¯ 3D æ©Ÿå™¨äºº (3 å€‹é—œç¯€)ï¼š$360^3 = 46,656,000$ å€‹è¼¸å‡ºï¼
2. **ã€Œè®“ç¶²è·¯è¼¸å‡ºæœ€ä½³å‹•ä½œã€**ï¼šé€™å°±æ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³• (PPO)ï¼ä¸æ˜¯ DQN äº†

**è§£æ±ºæ–¹æ¡ˆé è¦½**ï¼š
* ç­–ç•¥æ¢¯åº¦æ–¹æ³• (PPO/GRPO) ç›´æ¥å­¸ç¿’ $\pi(a|s)$ï¼Œå¯ä»¥è¼¸å‡ºé€£çºŒå‹•ä½œçš„åˆ†ä½ˆ

### 3.3 DQN é—œéµæŠ€è¡“ (Experience Replay & Target Network)

ç›´æ¥ç”¨ Q-Learning çš„ TD æ›´æ–°å…¬å¼ä¾†è¨“ç·´ç¥ç¶“ç¶²è·¯æ˜¯**æ¥µåº¦ä¸ç©©å®š**çš„ã€‚DQN å¼•å…¥äº†å…©é …é—œéµæŠ€è¡“ (ç©©å®šå™¨) ä¾†è§£æ±ºé€™å€‹å•é¡Œã€‚

**ç‚ºä»€éº¼ç›´æ¥å¥—ç”¨ Q-Learning æœƒå¤±æ•—ï¼Ÿ**
1. **è³‡æ–™ç›¸é—œæ€§**ï¼šé€£çºŒçš„ç¶“é©—é«˜åº¦ç›¸é—œï¼Œç ´å£ SGD çš„ i.i.d. å‡è¨­
2. **ç§»å‹•ç›®æ¨™**ï¼štarget å’Œ prediction ç”¨åŒä¸€å€‹ç¶²è·¯ï¼Œå°è‡´è¨“ç·´ä¸ç©©å®š
3. **ç½é›£æ€§éºå¿˜**ï¼šç¶²è·¯æœƒå¿˜è¨˜èˆŠç¶“é©—

**1. ç©©å®šå™¨ #1ï¼šç¶“é©—å›æ”¾ (Experience Replay)**
* **å•é¡Œ**ï¼šç¥ç¶“ç¶²è·¯è¨“ç·´æœ€æ€•ã€Œ**é«˜åº¦ç›¸é—œ (Correlated)**ã€çš„è³‡æ–™ã€‚å¦‚æœæˆ‘å€‘ç”¨é€£çºŒçš„éŠæˆ²ç¶“é©— `(s_t, s_{t+1}, s_{t+2}, ...)` ä¾†è¨“ç·´ï¼Œç¶²è·¯æœƒã€Œéåº¦æ“¬åˆ (Overfit)ã€æ–¼ç•¶å‰çš„éŠæˆ²å€åŸŸï¼Œä¸¦ã€Œ**å¿˜è¨˜**ã€å®ƒä»¥å‰å­¸åˆ°çš„æ±è¥¿ã€‚
* **è§£æ±ºæ–¹æ¡ˆ**ï¼šå»ºç«‹ä¸€å€‹ã€Œ**å›æ”¾ç·©è¡å€ (Replay Buffer)**ã€(ä¸€å€‹ `deque`)ã€‚
    1.  **æ”¶é›†**ï¼šAgent æ­£å¸¸ç©éŠæˆ²ï¼ŒæŠŠ**æ¯ä¸€**æ­¥çš„ç¶“é©— `(S, A, R, S')` å­˜é€² Replay Buffer (ä¾‹å¦‚å„²å­˜éå» 10 è¬æ­¥)ã€‚
    2.  **è¨“ç·´**ï¼šç•¶è¦è¨“ç·´ç¶²è·¯æ™‚ï¼Œæˆ‘å€‘**ä¸æ˜¯**ç”¨ã€Œå‰›å‰›çš„ç¶“é©—ã€ï¼Œè€Œæ˜¯å¾ Replay Buffer ä¸­**ã€Œéš¨æ©ŸæŠ½æ¨£ (random sample)ã€**ä¸€å°æ‰¹ (mini-batch) (ä¾‹å¦‚ 64 ç­†) **ä¸ç›¸é—œ**çš„èˆŠç¶“é©—ã€‚
* **å¥½è™•**ï¼šæ‰“ç ´äº†è³‡æ–™çš„ç›¸é—œæ€§ï¼Œè®“è¨“ç·´æ›´ç©©å®šã€‚

**2. ç©©å®šå™¨ #2ï¼šç›®æ¨™ç¶²è·¯ (Target Network)**
* **å•é¡Œ**ï¼šã€Œ**ç§»å‹•çš„é¶å¿ƒ (Moving Target)**ã€å•é¡Œã€‚
* **ç†è«–**ï¼šåœ¨ Q-Learning çš„æ›´æ–°ä¸­ï¼Œæˆ‘å€‘ç”¨ã€Œä¸€å€‹ç¶²è·¯ã€åŒæ™‚è¨ˆç®—**ã€Œé æ¸¬å€¼ã€**å’Œ**ã€Œç›®æ¨™å€¼ã€**ã€‚
    * $\text{TD Target} = R + \gamma \cdot \max Q_{\text{new}}(S', a')$
    * $\text{Loss} = (\text{TD Target} - Q_{\text{new}}(S, A))^2$
* **å•é¡Œ**ï¼šé€™å°±åƒä½  (ç¶²è·¯) åœ¨ç„æº–ä¸€å€‹é¶å¿ƒ (Target)ï¼Œä½†é¶å¿ƒä¹Ÿæ˜¯ç”±ä½  (ç¶²è·¯) æ±ºå®šçš„ã€‚ä½ ä¸€èª¿æ•´ç«™å§¿ (æ›´æ–°æ¬Šé‡)ï¼Œé¶å¿ƒä¹Ÿè·Ÿè‘—äº‚å‹•ï¼Œä½ æ°¸é ç„ä¸æº–ã€‚
* **è§£æ±ºæ–¹æ¡ˆ**ï¼šä½¿ç”¨**å…©å¥—**ç¥ç¶“ç¶²è·¯ã€‚
    1.  **ç·šä¸Šç¶²è·¯ (Online Network, $Q_{\text{online}}$)**ï¼š
        * é€™æ˜¯æˆ‘å€‘**ä¸»è¦**åœ¨è¨“ç·´çš„ç¶²è·¯ã€‚
        * å®ƒè² è²¬è¨ˆç®—ã€Œ**é æ¸¬å€¼**ã€$Q_{\text{online}}(S, A)$ã€‚
    2.  **ç›®æ¨™ç¶²è·¯ (Target Network, $Q_{\text{target}}$)**ï¼š
        * é€™æ˜¯ Online Network çš„ä¸€å€‹ã€Œ**è¤‡è£½é«”**ã€ï¼Œå®ƒçš„æ¬Šé‡æ˜¯**è¢«å‡çµçš„**ã€‚
        * å®ƒ**åª**è² è²¬è¨ˆç®—ã€Œ**ç›®æ¨™å€¼**ã€ $\text{TD Target} = R + \gamma \cdot \max Q_{\text{target}}(S', a')$ã€‚
* **é‹ä½œ**ï¼š
    1.  `Online Network` (å°„æ‰‹) å»è¿½é€ `Target Network` (**å›ºå®šçš„é¶å¿ƒ**)ï¼Œè¨“ç·´è®Šå¾—ç©©å®šã€‚
    2.  æ¯éš” `N` æ­¥ (ä¾‹å¦‚ 1000 æ­¥)ï¼Œæˆ‘å€‘æ‰ã€ŒåŒæ­¥ã€ä¸€æ¬¡ï¼šæŠŠ `Online Network` çš„**æ–°æ¬Šé‡**è¤‡è£½åˆ° `Target Network` ä¸Š (ç§»å‹•é¶å¿ƒ)ã€‚

**Target Network æ›´æ–°æ–¹å¼å°æ¯”**ï¼š

| æ›´æ–°æ–¹å¼ | å…¬å¼ | å„ªé» | ç¼ºé» | ä½¿ç”¨å ´æ™¯ |
|----------|------|------|------|----------|
| **Hard Update** | $\theta^{-} = \theta$ (æ¯ N æ­¥) | ç°¡å–®ã€ç©©å®š | æ›´æ–°æ™‚å¯èƒ½çªè®Š | DQN åŸè«–æ–‡ |
| **Soft Update (Polyak)** | $\theta^{-} \leftarrow \tau \theta + (1-\tau)\theta^{-}$ (æ¯æ­¥) | å¹³æ»‘ã€é€£çºŒ | éœ€èª¿æ•´ $\tau$ | DDPG, TD3, SAC |

**ä½ çš„å¯¦ä½œä½¿ç”¨**ï¼šHard Update (æ¯ `TARGET_UPDATE_FREQ` æ­¥)

```python
# Hard Update (DQN åŸè«–æ–‡)
if step % TARGET_UPDATE_FREQ == 0:
    target_state = nnx.state(online_network)
    nnx.update(target_network, target_state)

# Soft Update (ç¾ä»£æ–¹æ³•ï¼Œåƒè€ƒ)
# TAU = 0.005
# target_state = jax.tree_map(
#     lambda t, o: TAU * o + (1 - TAU) * t,
#     nnx.state(target_network),
#     nnx.state(online_network)
# )
# nnx.update(target_network, target_state)
```

### 3.4 DQN çš„éä¼°è¨ˆå•é¡Œ (Overestimation Bias)

é€™æ˜¯ DQN çš„ä¸€å€‹é‡è¦ç¼ºé™·ï¼Œå°è‡´äº† Double DQN çš„å‡ºç¾ã€‚

**å•é¡Œä¾†æº**ï¼š
$$
\text{TD Target} = R + \gamma \max_{a'} Q_{\text{target}}(S', a')
$$

**ç‚ºä»€éº¼ $\max$ æœƒå°è‡´éä¼°è¨ˆï¼Ÿ**

å‡è¨­çœŸå¯¦ Q å€¼éƒ½æ˜¯ 0ï¼Œä½†ç¶²è·¯çš„ä¼°è¨ˆæœ‰éš¨æ©Ÿèª¤å·®ï¼š
```
çœŸå¯¦ Q(s', aâ‚) = 0, ä¼°è¨ˆ = +0.5 (é«˜ä¼°)
çœŸå¯¦ Q(s', aâ‚‚) = 0, ä¼°è¨ˆ = -0.3 (ä½ä¼°)
çœŸå¯¦ Q(s', aâ‚ƒ) = 0, ä¼°è¨ˆ = +0.2 (é«˜ä¼°)
```

ç•¶æˆ‘å€‘å– $\max$ï¼š
```
max(+0.5, -0.3, +0.2) = +0.5
```

å•é¡Œï¼š**$\max$ æ“ä½œç³»çµ±æ€§åœ°é¸æ“‡äº†ã€Œè¢«é«˜ä¼°ã€çš„å‹•ä½œ**ï¼

**æ•¸å­¸è­‰æ˜**ï¼š
$$
\mathbb{E}[\max(X_1, X_2, ...)] \geq \max(\mathbb{E}[X_1], \mathbb{E}[X_2], ...)
$$

**è§£æ±ºæ–¹æ¡ˆï¼šDouble DQN (DDQN)**

å°‡ã€Œé¸æ“‡å‹•ä½œã€å’Œã€Œè©•ä¼°å‹•ä½œã€åˆ†é–‹ï¼š
```python
# DQN (æœ‰éä¼°è¨ˆ)
best_action = jnp.argmax(target_network(next_state))  # ç”¨ target é¸
td_target = reward + GAMMA * target_network(next_state)[best_action]  # ç”¨ target è©•ä¼°

# Double DQN (ä¿®æ­£éä¼°è¨ˆ)
best_action = jnp.argmax(online_network(next_state))  # ç”¨ online é¸
td_target = reward + GAMMA * target_network(next_state)[best_action]  # ç”¨ target è©•ä¼°
```

**æ•ˆæœ**ï¼šDDQN åœ¨å¤§å¤šæ•¸ Atari éŠæˆ²ä¸Šè¡¨ç¾å„ªæ–¼ DQNã€‚

### 3.5 DQN å¯¦ä½œ (Flax NNX)

æˆ‘å€‘ä½¿ç”¨ Flax NNX å¯¦ä½œäº† DQN Agent ä¾†è§£æ±ºã€ŒCartPole-v1ã€å•é¡Œã€‚

* **`QNetwork(nnx.Module)`**ï¼šæˆ‘å€‘ç”¨ `nnx.Linear` å»ºç«‹äº†ä¸€å€‹ 3 å±¤çš„ MLP ä½œç‚ºå‡½æ•¸è¿‘ä¼¼å™¨ã€‚
* **`ReplayBuffer(deque)`**ï¼šæˆ‘å€‘å¯¦ä½œäº† `add()` å’Œ `sample()` æ–¹æ³•ã€‚
* **`DQNAgent`**ï¼š
    * `__init__`ï¼šåˆå§‹åŒ–äº† `online_network` å’Œ `target_network`ã€‚
    * **é—œéµ API (Flax NNX)**ï¼š
        * ä½¿ç”¨ `nnx.Optimizer(model, optax.adam(...))` ä¾†å°‡ `optax` å„ªåŒ–å™¨èˆ‡æ¨¡å‹**ç¶å®š**ã€‚
        * ä½¿ç”¨ `nnx.state(online_model)` ä¾†**æå–**æ¬Šé‡ã€‚
        * ä½¿ç”¨ `nnx.update(target_model, online_state)` ä¾†**è¤‡è£½**æ¬Šé‡ (å¯¦ç¾ `Target Network` åŒæ­¥)ã€‚
* **`train_step` (è¨“ç·´æ­¥é©Ÿ)**ï¼š
    1.  å¾ `ReplayBuffer` ä¸­ `sample()` ä¸€å€‹ `batch`ã€‚
    2.  **è¨ˆç®— Target**ï¼š`td_target = batch_rewards + GAMMA * jnp.max(self.target_network(batch_next_states), axis=1)`
    3.  **è¨ˆç®— Loss**ï¼šå®šç¾© `loss_fn`ï¼Œè¨ˆç®—ã€Œé æ¸¬å€¼ã€(ä¾†è‡ª `self.online_network`) å’Œ `td_target` ä¹‹é–“çš„**å‡æ–¹èª¤å·® (MSE)**ã€‚
    4.  **æ›´æ–°**ï¼šä½¿ç”¨ `nnx.value_and_grad` å’Œ `self.optimizer.update(grads)` ä¾†æ›´æ–° `online_network`ã€‚

### 3.6 ReplayBuffer å¯¦ä½œç´°ç¯€

**é—œéµå¯¦ä½œå•é¡Œ**ï¼š

1. **Buffer æœªæ»¿æ™‚çš„æ¡æ¨£**
```python
from collections import deque

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)  # è‡ªå‹•æ·˜æ±°èˆŠè³‡æ–™

    def sample(self, batch_size):
        # å•é¡Œï¼šå¦‚æœ len(buffer) < batch_size æ€éº¼è¾¦ï¼Ÿ
        batch_size = min(batch_size, len(self.buffer))  # å‹•æ…‹èª¿æ•´
        return random.sample(self.buffer, batch_size)
```

2. **è¨˜æ†¶é«”ç®¡ç†**
* `deque(maxlen=N)` æœƒè‡ªå‹•åˆªé™¤æœ€èˆŠçš„è³‡æ–™
* CartPoleï¼š10000 transitions â‰ˆ 10KB
* Atariï¼š100è¬ transitions (84Ã—84Ã—4 åœ–ç‰‡) â‰ˆ 30GBï¼

3. **å„ªå…ˆç¶“é©—å›æ”¾ (Prioritized Experience Replay, PER)**

æ¨™æº– Replay Buffer çš„å•é¡Œï¼šæ‰€æœ‰ç¶“é©—è¢«å¹³ç­‰å°å¾…ã€‚

**PER çš„æ”¹é€²**ï¼šæ ¹æ“š **TD Error** çš„å¤§å°ä¾†æ¡æ¨£
```python
# TD Error å¤§ â†’ é€™å€‹ç¶“é©—ã€Œå¾ˆæ„å¤–ã€â†’ å­¸ç¿’åƒ¹å€¼é«˜
priority = abs(td_error) + epsilon  # epsilon é¿å… priority = 0

# æ¡æ¨£æ©Ÿç‡æ­£æ¯”æ–¼ priority
p_i = priority_i^Î± / Î£(priority_j^Î±)
```

**æ•ˆæœ**ï¼šPER åœ¨ Atari ä¸Šæå‡ DQN ç´„ 2 å€æ¨£æœ¬æ•ˆç‡ï¼

### 3.7 DQN å®¶æ—æ¼”ç®—æ³•é€²åŒ–åœ–

```
DQN (2013, DeepMind)
â”œâ”€â”€ Double DQN (2015) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¿®æ­£éä¼°è¨ˆ
â”œâ”€â”€ Prioritized DQN (2015) â”€â”€â”€â”€â”€â”€ æ›´è°æ˜çš„æ¡æ¨£
â”œâ”€â”€ Dueling DQN (2016) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ”¹é€²ç¶²è·¯æ¶æ§‹
â””â”€â”€ Rainbow DQN (2017) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ•´åˆæ‰€æœ‰æ”¹é€²
    â””â”€â”€ æ•ˆæœï¼šAtari ä¸Šè¶…è¶Šäººé¡æ°´æº–
```

**Rainbow DQN æ•´åˆäº† 6 é …æŠ€è¡“**ï¼š
1. Double DQN
2. Prioritized Replay
3. Dueling Networks
4. Multi-step Returns (n-step TD)
5. Distributional RL (C51)
6. Noisy Nets (æ¢ç´¢ç­–ç•¥)

### 3.8 DQN è¶…åƒæ•¸èˆ‡èª¿è©¦æŠ€å·§

**é—œéµè¶…åƒæ•¸è¡¨**ï¼š

| è¶…åƒæ•¸ | å…¸å‹å€¼ | ä½œç”¨ | CartPole | Atari |
|--------|--------|------|----------|-------|
| **Buffer Size** | 10K - 1M | Replay Buffer å®¹é‡ | 10K | 1M |
| **Batch Size** | 32 - 256 | è¨“ç·´æ‰¹æ¬¡å¤§å° | 64 | 32 |
| **Learning Rate** | 1e-4 - 1e-3 | Adam å­¸ç¿’ç‡ | 1e-3 | 2.5e-4 |
| **Î³ (Gamma)** | 0.99 | æŠ˜æ‰£å› å­ | 0.99 | 0.99 |
| **Îµ (Epsilon)** | 1.0 â†’ 0.01 | æ¢ç´¢ç‡ | è¡°æ¸› | è¡°æ¸› |
| **Target Update** | 100 - 10K | ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡ | 100 | 10K |

**å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ**ï¼š

| å•é¡Œ | ç—‡ç‹€ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|----------|
| **Q å€¼çˆ†ç‚¸** | Q å€¼ > 1000 | å­¸ç¿’ç‡å¤ªå¤§ | é™ä½ LR æˆ–ä½¿ç”¨ gradient clipping |
| **ä¸å­¸ç¿’** | çå‹µä¸ä¸Šå‡ | æ¢ç´¢ä¸è¶³ | å¢åŠ  Îµ æˆ–è¨“ç·´æ™‚é–“ |
| **éæ“¬åˆ** | è¨“ç·´å¥½æ¸¬è©¦å·® | Buffer å¤ªå° | å¢å¤§ Buffer Size |
| **è¨“ç·´ä¸ç©©å®š** | çå‹µåŠ‡çƒˆéœ‡ç›ª | Target æ›´æ–°å¤ªé »ç¹ | å¢å¤§ Target Update Freq |

**èª¿è©¦æŠ€å·§**ï¼š
1. **ç›£æ§ Q å€¼åˆ†ä½ˆ**ï¼šæ‡‰è©²é€æ¼¸å¢å¤§ä¸¦ç©©å®š
2. **ç›£æ§ TD Error**ï¼šæ‡‰è©²é€æ¼¸æ¸›å°
3. **ç›£æ§ Loss**ï¼šMSE Loss æ‡‰è©²ä¸‹é™
4. **è¦–è¦ºåŒ– Buffer**ï¼šç¢ºä¿åŒ…å«å¤šæ¨£åŒ–çš„ç¶“é©—

ğŸ“ **å®Œæ•´å¯¦ä½œç¨‹å¼ç¢¼**ï¼š[`3.1.Cart_Pole_DQN/cart_pole_dqn.py`](3.1.Cart_Pole_DQN/cart_pole_dqn.py)
ğŸ“– **è©³ç´°èªªæ˜æ–‡ä»¶**ï¼š[`3.1.Cart_Pole_DQN/README.md`](3.1.Cart_Pole_DQN/README.md)

---

[â¬…ï¸ ä¸Šä¸€ç« ï¼šæ¨¡çµ„äºŒ - Q-Learning](../2.Q_Learning/README.md) | [è¿”å›ç›®éŒ„](../README.md) | [ä¸‹ä¸€ç« ï¼šæ¨¡çµ„å›› - Policy Gradients â¡ï¸](../4.Policy_Gradients/README.md)