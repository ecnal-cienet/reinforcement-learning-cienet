## <a name="module-1-the-foundations-of-rl"></a>模組一：RL 的核心概念

[⬅️ 上一章：安裝指南](../0.Setup/README.md) | [返回目錄](../README.md) | [下一章：模組二 - Q-Learning ➡️](../2.Q_Learning/README.md)

---

本模組介紹了 RL 的基本「世界觀」和共同詞彙。

### 1.1 核心 RL 迴圈 (The Core Loop)

RL 的一切都基於「**智慧體 (Agent)**」和「**環境 (Environment)**」之間的互動迴圈。

1.  **智慧體 (Agent)**：學習者或決策者 (例如：遊戲角色)。
2.  **環境 (Environment)**：Agent 互動的外部世界 (例如：遊戲關卡)。
3.  **狀態 (State, $S$)**：環境在某一瞬間的快照 (例如：角色的 $(x, y)$ 座標)。
4.  **動作 (Action, $A$)**：Agent 根據 $S$ 能做出的選擇 (例如：「向左」)。
5.  **獎勵 (Reward, $R$)**：Agent 執行 $A$ 後，環境給予的「回饋訊號」 (例如：吃到金幣 `+10`，碰到敵人 `-50`)。

**Agent 的唯一目標**：最大化「**未來的累積總獎勵 (Cumulative Future Reward)**」。

**RL 的兩大任務類型**：
* **回合式任務 (Episodic Tasks)**：有明確的「結束」點 (例如：遊戲勝利/失敗、到達目標)。
    * 例子：圍棋 (一局結束)、CartPole (桿子倒下)、走迷宮 (到達出口)
    * 一個完整的回合稱為 **Episode (回合)**
    * 環境會給出 `done=True` 信號表示回合結束
* **持續式任務 (Continuing Tasks)**：永遠不會結束。
    * 例子：股票交易機器人、推薦系統
    * 需要折扣因子 (下節說明) 來避免無限累積獎勵

### 1.1.1 關鍵詞彙區分 (易混淆！)

這是初學者最容易混淆的概念，請務必仔細區分：

| 概念 | 符號 | 定義 | 例子 |
|------|------|------|------|
| **獎勵 (Reward)** | $R_t$ | **單一**時間步的即時獎勵 | 吃到金幣 `+10` |
| **回報 (Return)** | $G_t$ | 從時間 $t$ 開始的**累積折扣獎勵** | $G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ...$ |
| **價值 (Value)** | $V(s)$ | 狀態 $s$ 的**期望回報** | $V(s) = \mathbb{E}[G_t \mid S_t=s]$ |

**記憶技巧**：
* Reward = 「當下的錢」
* Return = 「這一局的總收益」
* Value = 「這個位置的平均總收益」

### 1.1.2 折扣因子 (Discount Factor, $\gamma$)

為什麼我們需要「折扣」未來的獎勵？

**數學定義**：
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$

其中 $\gamma \in [0, 1]$ 稱為「折扣因子」。

**直覺理解**：
* **$\gamma = 0$**：「**極度短視**」— Agent 只在乎「現在」的獎勵。
    * $G_t = R_t$ (完全忽略未來)
    * 例子：貪吃蛇只吃眼前的食物，不管會不會撞牆
* **$\gamma = 1$**：「**完全遠視**」— Agent 平等看待所有未來獎勵。
    * $G_t = R_t + R_{t+1} + R_{t+2} + ...$ (可能無限大！)
    * 問題：對於持續式任務，總獎勵可能發散
* **$\gamma = 0.9 \sim 0.99$**：「**平衡視角**」(實務中最常用)
    * 近期獎勵重要，但也會考慮長期後果
    * 例如：$\gamma=0.99$ 時，100 步後的獎勵只剩原本的 $0.99^{100} \approx 0.366$ 倍

**為什麼需要折扣？**
1. **數學穩定性**：確保無限時間的總獎勵收斂 (不會變成無限大)
2. **不確定性**：未來越遠越不確定，應該給較小的權重
3. **控制時間視野**：$\gamma$ 控制 Agent 的「計劃深度」

**典型值**：
* 簡單任務 (Q-Learning)：$\gamma = 0.9$
* 複雜任務 (DQN/PPO)：$\gamma = 0.99$ 或 $0.999$

### 1.2 馬可夫決策過程 (MDPs)

MDPs 是描述 RL 迴圈的數學框架。其核心假設是「**馬可夫特性 (Markov Property)**」。

> **馬可夫特性 (又稱「無記憶性」)**：
> 未來的狀態**只**取決於「現在的狀態」和「現在的動作」，而與「過去如何到達現在這個狀態」完全無關。

換句話說，**「現在的狀態 $S$」已經包含了所有做決策所需的歷史資訊**。

**範例分析**：
* **✅ 符合馬可夫特性：** 圍棋
    * 當前棋盤佈局 = 完整狀態
    * 不需要知道「這顆子是第幾手下的」
* **✅ 符合馬可夫特性：** CartPole
    * 狀態 = `(桿子角度, 桿子角速度, 車子位置, 車子速度)`
    * 這 4 個數字完整描述了系統的物理狀態
* **❌ 看似不符合：** 撲克牌
    * 只看「當前手牌」不夠，需要記憶對手過去的下注模式
    * **但可以修正！** 將「過去 N 輪的下注歷史」納入狀態定義
    * 新狀態 = `(當前手牌, 過去 5 輪下注記錄, 對手表情)` → 變成馬可夫！

**關鍵洞察**：
> 幾乎所有問題都可以透過「**擴充狀態定義**」來滿足馬可夫特性。
> 如果需要歷史資訊，就把歷史納入狀態！

**部分觀察 MDP (POMDP)**：
* 有些情況下，Agent 無法完全觀察到真實狀態
* 例子：機器人只有前方相機，看不到背後
* 解決方案：用 RNN/Transformer 等記憶機制來「估算」完整狀態

### 1.3 策略 (Policy) 與價值函數 (Value Function)

這兩個概念描述了 Agent 的「大腦」和「目標」。

* **策略 (Policy, $\pi$)**：
    * Agent 的「行為準則」或「決策大腦」。
    * 它是一個函式，決定了在特定狀態 $S$ 下該採取哪個動作 $A$。
    * $\pi(A | S)$ = 在 $S$ 狀態下，執行 $A$ 動作的機率。
    * **我們的目標**：找到「**最佳策略 ($\pi^*$)**」，以獲得最高總獎勵。

* **價值函數 (Value Function, $V(s)$)**：
    * 一個「評價函式」，用來衡量「**一個狀態 $S$ 有多好**」。
    * $V(s)$ = 「從 $S$ 狀態出發，並遵循策略 $\pi$ 到底，**預期**能獲得的未來總獎勵。」
    * Agent 會利用 $V(s)$ 來改進 $\pi$ (例如：選擇那個能帶我走向「更高價值 $V(s')$」的動作 $A$)。

**策略的兩種類型**：
* **確定性策略 (Deterministic Policy)**：$a = \pi(s)$
    * 給定狀態 $s$，永遠輸出固定動作 $a$
    * 例子：「在 (2,3) 位置永遠往右走」
* **隨機性策略 (Stochastic Policy)**：$\pi(a|s)$
    * 給定狀態 $s$，輸出動作的**機率分佈**
    * 例子：「在 (2,3) 位置，70% 往右，30% 往上」
    * **現代深度 RL (PPO/GRPO) 都使用隨機策略！**

**價值函數的完整定義**：
$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[ G_t \mid S_t = s \right] = \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k} \mid S_t = s \right]
$$

這個公式的意思是：
* 「如果我現在在狀態 $s$」
* 「並且之後一直遵循策略 $\pi$ 來選擇動作」
* 「那麼我期望能獲得的總回報是多少？」

### 1.4 探索 (Exploration) vs. 利用 (Exploitation)

這是 Agent 學習時面臨的核心兩難。

* **利用 (Exploitation)**：
    * **定義**：根據**目前所知**的資訊，做出「最好」的選擇。
    * **例子**：去你最愛的那家餐廳 (你知道它 90 分)。
* **探索 (Exploration)**：
    * **定義**：**故意**嘗試一些「未知」的選擇，目的是為了「收集新資訊」。
    * **例子**：嘗試一家新餐廳 (可能是 10 分，也可能是 100 分)。

一個好的 Agent 必須在這兩者間取得平衡。**Epsilon-Greedy ($\epsilon$-Greedy)** 是最常見的策略：
* 有 $1-\epsilon$ 的機率 (例如 90%) 去「利用」。
* 有 $\epsilon$ 的機率 (例如 10%) 去「探索」。

**常見的探索策略**：

| 策略 | 描述 | 適用方法 |
|------|------|----------|
| **$\epsilon$-Greedy** | 以 $\epsilon$ 機率隨機選動作，否則選最佳動作 | Q-Learning, DQN |
| **$\epsilon$ 衰減** | $\epsilon$ 從 1.0 逐漸降到 0.01 (先探索後利用) | DQN |
| **Softmax / Boltzmann** | 根據 Q 值的 softmax 機率來選擇動作 | 表格型方法 |
| **Entropy Bonus** | 在 loss 中加入熵項，鼓勵策略保持隨機性 | PPO, GRPO |
| **Intrinsic Motivation** | 給予「好奇心」獎勵 (例如到達新狀態) | 複雜探索問題 |

**$\epsilon$ 衰減範例** (DQN 常用)：
```python
epsilon_start = 1.0      # 訓練初期：100% 探索
epsilon_end = 0.01       # 訓練後期：1% 探索
epsilon_decay = 0.995    # 每個 episode 衰減

epsilon = max(epsilon_end, epsilon * epsilon_decay)
```

### 1.5 RL 方法的兩大家族 (預覽)

這是整個課程的「地圖」，幫助你理解後續章節的定位：

```
強化學習演算法
├── 價值為基礎 (Value-Based)
│   ├── Q-Learning (模組二)          ← 表格型
│   └── DQN (模組三)                 ← 深度學習
│       └── 局限：無法處理連續動作空間
│
└── 策略為基礎 (Policy-Based)
    ├── REINFORCE (模組四)           ← 基礎策略梯度
    ├── Actor-Critic (模組五)        ← 加入價值函數當基線
    ├── PPO (模組五)                 ← 加入安全鎖
    └── GRPO (模組七)                ← 移除 Critic 省記憶體
```

**兩大家族的核心差異**：

| 特性 | 價值為基礎 (DQN) | 策略為基礎 (PPO/GRPO) |
|------|-----------------|----------------------|
| **學習目標** | Q 函數 $Q(s,a)$ | 策略 $\pi(a\|s)$ |
| **動作選擇** | $\arg\max_a Q(s,a)$ | 從 $\pi(a\|s)$ 採樣 |
| **連續動作** | ❌ 無法處理 | ✅ 可以處理 |
| **資料效率** | ✅ 高 (off-policy) | ❌ 低 (on-policy) |
| **訓練穩定性** | 中等 | 高 (有安全鎖) |
| **應用場景** | 遊戲 (Atari) | 機器人、LLM |

---

[⬅️ 上一章：安裝指南](../0.Setup/README.md) | [返回目錄](../README.md) | [下一章：模組二 - Q-Learning ➡️](../2.Q_Learning/README.md)